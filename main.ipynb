{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using NMT to process Twitter data\n",
    "I developed an app that listens to Twitter feeds and saves the response and original Tweet to a Sqlite3 database.  I then used the data as input for this tutorial:  https://github.com/tensorflow/nmt.  I had to do a few cleanup steps first to get the data loaded, but it seems to be working now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set environment variables and save to a script\n",
    "This is the only way I could figure out how to do this so the variables would be available in each cell.  Basically I write a file called env.sh and reference that in every subsequent bash cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting env.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile env.sh\n",
    "export HOME_DIR=`pwd`\n",
    "export DATA_DIR=$HOME_DIR/data_files\n",
    "export MODEL_DIR=$HOME_DIR/nmt_model\n",
    "export NMT_DIR=$HOME_DIR/nmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete old directories if they exist\n",
    "In later cells, we're going to recreate these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing /home/rich/src/concretely/concretely_nmt/data_files\n",
      "removing /home/rich/src/concretely/concretely_nmt/nmt_model\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    ". ./env.sh\n",
    "if [ -d \"$DATA_DIR\" ]; then\n",
    "    echo removing $DATA_DIR\n",
    "    rm -r $DATA_DIR\n",
    "fi\n",
    "\n",
    "if [ -d \"$MODEL_DIR\" ]; then\n",
    "    echo removing $MODEL_DIR\n",
    "    rm -r $MODEL_DIR\n",
    "fi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkout the nmt project from GitHub\n",
    "Only do this if the directory doesn't exist yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    ". ./env.sh\n",
    "mkdir $DATA_DIR\n",
    "mkdir $MODEL_DIR\n",
    "\n",
    "if [ ! -d \"$NMT_DIR\" ]; then\n",
    "  git clone https://github.com/tensorflow/nmt/\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the database\n",
    "This program reads the sqlite3 tables and extracts the Tweet text from the Tweet json and does a little cleanup.  Right now, input and output folders are hardcoded, but I plan on fixing that.  It also splits the data into train, eval and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.txt\n",
      "output.txt\n",
      "train.from\n",
      "train.to\n",
      "tst2012.from\n",
      "tst2012.to\n",
      "tst2013.from\n",
      "tst2013.to\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    ". ./env.sh\n",
    "python $HOME_DIR/process_db.py\n",
    "ls $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate vocab files\n",
    "This program generates vocabulary files for the input and output files.  I plan on either updating the generate_vocab.py or merging with process_db.py to get rid of all this sed stuff.  I was having issues receiving duplicates and blank lines, so for now I used these tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.txt\n",
      "output.txt\n",
      "train.from\n",
      "train.to\n",
      "tst2012.from\n",
      "tst2012.to\n",
      "tst2013.from\n",
      "tst2013.to\n",
      "vocab.from\n",
      "vocab.to\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    ". ./env.sh\n",
    "python generate_vocab.py --max_vocab_size=10000 --downcase=True $DATA_DIR/train.from | sed 's/[[:punct:]]//g; s/[[:space:]]/\\n/g' | sort | uniq | sed '/^[[:space:]]*$/d' > $DATA_DIR/vocab.from\n",
    "python generate_vocab.py --max_vocab_size=10000 --downcase=True $DATA_DIR/train.to | sed 's/[[:punct:]]//g; s/[[:space:]]/\\n/g' | sort | uniq | sed '/^[[:space:]]*$/d' > $DATA_DIR/vocab.to\n",
    "ls $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "This is the line to actually run the model.  Note that num_train_steps=5.  This is just to verify that the code runs.  Originally it was set to 12000, this like the other parameters can be modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Job id 0\n",
      "# Devices visible to TensorFlow: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 726199474441383584), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 3929896018156925669)]\n",
      "# Vocab file /home/rich/src/concretely/concretely_nmt/data_files/vocab.from exists\n",
      "The first 3 vocab words [ãƒ„, ðŸ¤¬, ðŸ¤£ðŸ¤£] are not [<unk>, <s>, </s>]\n",
      "# Vocab file /home/rich/src/concretely/concretely_nmt/data_files/vocab.to exists\n",
      "The first 3 vocab words [ðŸ¤£, ðŸ¤£ðŸ¤£, 0033] are not [<unk>, <s>, </s>]\n",
      "  saving hparams to /home/rich/src/concretely/concretely_nmt/nmt_model/hparams\n",
      "  saving hparams to /home/rich/src/concretely/concretely_nmt/nmt_model/best_bleu/hparams\n",
      "  attention=\n",
      "  attention_architecture=standard\n",
      "  avg_ckpts=False\n",
      "  batch_size=128\n",
      "  beam_width=0\n",
      "  best_bleu=0\n",
      "  best_bleu_dir=/home/rich/src/concretely/concretely_nmt/nmt_model/best_bleu\n",
      "  check_special_token=True\n",
      "  colocate_gradients_with_ops=True\n",
      "  decay_scheme=\n",
      "  dev_prefix=/home/rich/src/concretely/concretely_nmt/data_files/tst2012\n",
      "  dropout=0.2\n",
      "  embed_prefix=None\n",
      "  encoder_type=uni\n",
      "  eos=</s>\n",
      "  epoch_step=0\n",
      "  forget_bias=1.0\n",
      "  infer_batch_size=32\n",
      "  infer_mode=greedy\n",
      "  init_op=uniform\n",
      "  init_weight=0.1\n",
      "  language_model=False\n",
      "  learning_rate=1.0\n",
      "  length_penalty_weight=0.0\n",
      "  log_device_placement=False\n",
      "  max_gradient_norm=5.0\n",
      "  max_train=0\n",
      "  metrics=['bleu']\n",
      "  num_buckets=5\n",
      "  num_dec_emb_partitions=0\n",
      "  num_decoder_layers=2\n",
      "  num_decoder_residual_layers=0\n",
      "  num_embeddings_partitions=0\n",
      "  num_enc_emb_partitions=0\n",
      "  num_encoder_layers=2\n",
      "  num_encoder_residual_layers=0\n",
      "  num_gpus=1\n",
      "  num_inter_threads=0\n",
      "  num_intra_threads=0\n",
      "  num_keep_ckpts=5\n",
      "  num_sampled_softmax=0\n",
      "  num_train_steps=5\n",
      "  num_translations_per_input=1\n",
      "  num_units=128\n",
      "  optimizer=sgd\n",
      "  out_dir=/home/rich/src/concretely/concretely_nmt/nmt_model\n",
      "  output_attention=True\n",
      "  override_loaded_hparams=False\n",
      "  pass_hidden_state=True\n",
      "  random_seed=None\n",
      "  residual=False\n",
      "  sampling_temperature=0.0\n",
      "  share_vocab=False\n",
      "  sos=<s>\n",
      "  src=from\n",
      "  src_embed_file=\n",
      "  src_max_len=50\n",
      "  src_max_len_infer=None\n",
      "  src_vocab_file=/home/rich/src/concretely/concretely_nmt/nmt_model/vocab.from\n",
      "  src_vocab_size=6185\n",
      "  steps_per_external_eval=None\n",
      "  steps_per_stats=100\n",
      "  subword_option=\n",
      "  test_prefix=/home/rich/src/concretely/concretely_nmt/data_files/tst2013\n",
      "  tgt=to\n",
      "  tgt_embed_file=\n",
      "  tgt_max_len=50\n",
      "  tgt_max_len_infer=None\n",
      "  tgt_vocab_file=/home/rich/src/concretely/concretely_nmt/nmt_model/vocab.to\n",
      "  tgt_vocab_size=4469\n",
      "  time_major=True\n",
      "  train_prefix=/home/rich/src/concretely/concretely_nmt/data_files/train\n",
      "  unit_type=lstm\n",
      "  use_char_encode=False\n",
      "  vocab_prefix=/home/rich/src/concretely/concretely_nmt/data_files/vocab\n",
      "  warmup_scheme=t2t\n",
      "  warmup_steps=0\n",
      "# Creating train graph ...\n",
      "# Build a basic encoder\n",
      "  num_layers = 2, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  learning_rate=1, warmup_steps=0, warmup_scheme=t2t\n",
      "  decay_scheme=, start_decay_step=5, decay_steps 0, decay_factor 1\n",
      "# Trainable variables\n",
      "Format: <name>, <shape>, <(soft) device placement>\n",
      "  embeddings/encoder/embedding_encoder:0, (6185, 128), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (4469, 128), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 4469), /device:GPU:0\n",
      "# Creating eval graph ...\n",
      "# Build a basic encoder\n",
      "  num_layers = 2, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "# Trainable variables\n",
      "Format: <name>, <shape>, <(soft) device placement>\n",
      "  embeddings/encoder/embedding_encoder:0, (6185, 128), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (4469, 128), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 4469), /device:GPU:0\n",
      "# Creating infer graph ...\n",
      "# Build a basic encoder\n",
      "  num_layers = 2, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  decoder: infer_mode=greedybeam_width=0, length_penalty=0.000000\n",
      "# Trainable variables\n",
      "Format: <name>, <shape>, <(soft) device placement>\n",
      "  embeddings/encoder/embedding_encoder:0, (6185, 128), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (4469, 128), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 4469), \n",
      "# log_file=/home/rich/src/concretely/concretely_nmt/nmt_model/log_1547017816\n",
      "  created train model with fresh parameters, time 0.16s\n",
      "  created infer model with fresh parameters, time 0.06s\n",
      "  # 39\n",
      "    src: AtAmazonHelp anyone care to explain to me why the people at the distribution center thought this was an acceptable way to package anything at all let alone $400 equipment? Im not normally one to complain but come on, this is just ridiculous... https://t.co/UP1bYJNXkj\n",
      "    ref: AtCustomerName Thank you for bringing this to our attention, Brandon. We're always looking for ways to improve, and we'll be sure to pass this along to our Packaging Team for improvement. ^KL\n",
      "    nmt: certificates suggest suggest sah searched searched searched active active active wishing wishing wishing wishing takenrc takenrc takenrc takenrc takenrc takenrc takenrc expedited expedited expedited expedited perhaps perhaps happenedrs expedited security security tm can accountma dealsbz dealsbz dealsbz dealsbz dealsbz dealsbz hasnt hasnt hasnt hasnt optionsinfo <s> <s> <s> <s> <s> came came came suitable came websiteapp preserve algunas vixx vixx reflected reflected reflected kyle designated designated designated land encountered designated encountered encountered encountered designated encountered arise encountered certainly certainly certainly certainly certainly unlock clarification clarification unlock\n",
      "  created eval model with fresh parameters, time 0.06s\n",
      "  eval dev: perplexity 4464.02, time 0s, Wed Jan  9 01:10:17 2019.\n",
      "  eval test: perplexity 4462.97, time 0s, Wed Jan  9 01:10:17 2019.\n",
      "  created infer model with fresh parameters, time 0.05s\n",
      "# Start step 0, lr 1, Wed Jan  9 01:10:17 2019\n",
      "# Init train iterator, skipping 0 elements\n",
      "  loaded infer model parameters from /home/rich/src/concretely/concretely_nmt/nmt_model/translate.ckpt-5, time 0.03s\n",
      "  # 56\n",
      "    src: AtAmazonHelpIN You are losing credibility, even after talking to your customer care executive I haven't received any purchased good from you and you have updated item delivered in my a/c. What is actual status? If can not send then cancel the order and refund me. It's way to business? https://t.co/xjAcTwiDdK\n",
      "    ref: AtCustomerName Please donâ€™t provide your order details as we consider them to be personal information. Our Twitter page is visible to public. (2/2) ^YB\n",
      "    nmt: to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to\n",
      "  loaded eval model parameters from /home/rich/src/concretely/concretely_nmt/nmt_model/translate.ckpt-5, time 0.03s\n",
      "  eval dev: perplexity 4811.75, time 0s, Wed Jan  9 01:10:23 2019.\n",
      "  eval test: perplexity 4697.86, time 0s, Wed Jan  9 01:10:23 2019.\n",
      "  loaded infer model parameters from /home/rich/src/concretely/concretely_nmt/nmt_model/translate.ckpt-5, time 0.02s\n",
      "# External evaluation, global step 5\n",
      "  decoding to output /home/rich/src/concretely/concretely_nmt/nmt_model/output_dev\n",
      "  done, num sentences 70, num translations per input 1, time 0s, Wed Jan  9 01:10:23 2019.\n",
      "  bleu dev: 0.0\n",
      "  saving hparams to /home/rich/src/concretely/concretely_nmt/nmt_model/hparams\n",
      "# External evaluation, global step 5\n",
      "  decoding to output /home/rich/src/concretely/concretely_nmt/nmt_model/output_test\n",
      "  done, num sentences 74, num translations per input 1, time 0s, Wed Jan  9 01:10:24 2019.\n",
      "  bleu test: 0.0\n",
      "  saving hparams to /home/rich/src/concretely/concretely_nmt/nmt_model/hparams\n",
      "# Final, step 5 lr 1 step-time 0.00s wps 0.00K ppl 0.00 gN 0.00 dev ppl 4811.75, dev bleu 0.0, test ppl 4697.86, test bleu 0.0, Wed Jan  9 01:10:24 2019\n",
      "# Done training!, time 6s, Wed Jan  9 01:10:24 2019.\n",
      "# Start evaluating saved best models.\n",
      "  created infer model with fresh parameters, time 0.06s\n",
      "  # 68\n",
      "    src: AtAmazonHelp AtAmazonHelp How exactly is a package that I ordered last week, that was supposed to get here by tomorrow going to get to me by tomorrow when the USPS has not received it yet? I am a PRIME member. Maybe you meant ship by tomorrow? #TRUTH #LIES\n",
      "    ref: AtCustomerName Hi Steve, what's the latest info on the tracking?^RS\n",
      "    nmt: arrivesrs arrivesrs accessibility accessibility accessibility force force force tedious tedious tedious tedious told mi mi mi mi youtube usual usual usual usual thisbz thisbz thisbz promitional promitional promitional usual dispatchedcn usual usual usual usual promitional promitional settings settings settings mystery mystery having having misunderstanding misunderstanding misunderstanding misunderstanding agreements 12gd 12gd this this this gst gst gst voucher voucher voucher voucher voucher invested invested gotcha gotcha invested invested invested straight couk dvds dvds dvds dvds bulbs bulbs rn rn carol carol sortedbz sortedbz sortedbz sortedbz shona shona notes notes every every every niece niece niece conversation conversation\n",
      "  created eval model with fresh parameters, time 0.07s\n",
      "  eval dev: perplexity 4465.24, time 0s, Wed Jan  9 01:10:25 2019.\n",
      "  eval test: perplexity 4464.33, time 0s, Wed Jan  9 01:10:25 2019.\n",
      "  created infer model with fresh parameters, time 0.06s\n",
      "  bleu dev: 0.0\n",
      "  bleu test: 0.0\n",
      "# Best bleu, step 0 lr 1 step-time 0.00s wps 0.00K ppl 0.00 gN 0.00 dev ppl 4465.24, dev bleu 0.0, test ppl 4464.33, test bleu 0.0, Wed Jan  9 01:10:25 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-09 01:10:13.754518: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "/home/rich/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING:tensorflow:From /home/rich/src/concretely/concretely_nmt/nmt/nmt/utils/iterator_utils.py:235: group_by_window (from tensorflow.contrib.data.python.ops.grouping) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.group_by_window(...)`.\n",
      "WARNING:tensorflow:From /home/rich/src/concretely/concretely_nmt/nmt/nmt/model_helper.py:402: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "2019-01-09 01:10:17.887626: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file /home/rich/src/concretely/concretely_nmt/nmt_model/vocab.to is already initialized.\n",
      "2019-01-09 01:10:17.887626: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file /home/rich/src/concretely/concretely_nmt/nmt_model/vocab.from is already initialized.\n",
      "2019-01-09 01:10:17.887626: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file /home/rich/src/concretely/concretely_nmt/nmt_model/vocab.to is already initialized.\n",
      "2019-01-09 01:10:19.194409: W tensorflow/core/framework/allocator.cc:122] Allocation of 112118272 exceeds 10% of system memory.\n",
      "2019-01-09 01:10:21.344972: W tensorflow/core/framework/allocator.cc:122] Allocation of 112118272 exceeds 10% of system memory.\n",
      "2019-01-09 01:10:22.854286: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file /home/rich/src/concretely/concretely_nmt/nmt_model/vocab.to is already initialized.\n",
      "2019-01-09 01:10:22.854345: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file /home/rich/src/concretely/concretely_nmt/nmt_model/vocab.to is already initialized.\n",
      "2019-01-09 01:10:22.854334: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file /home/rich/src/concretely/concretely_nmt/nmt_model/vocab.from is already initialized.\n",
      "2019-01-09 01:10:22.926037: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file /home/rich/src/concretely/concretely_nmt/nmt_model/vocab.from is already initialized.\n",
      "2019-01-09 01:10:22.926044: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file /home/rich/src/concretely/concretely_nmt/nmt_model/vocab.to is already initialized.\n",
      "2019-01-09 01:10:22.926044: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file /home/rich/src/concretely/concretely_nmt/nmt_model/vocab.to is already initialized.\n",
      "2019-01-09 01:10:23.446696: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file /home/rich/src/concretely/concretely_nmt/nmt_model/vocab.to is already initialized.\n",
      "2019-01-09 01:10:23.446712: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file /home/rich/src/concretely/concretely_nmt/nmt_model/vocab.from is already initialized.\n",
      "2019-01-09 01:10:23.446696: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file /home/rich/src/concretely/concretely_nmt/nmt_model/vocab.to is already initialized.\n",
      "2019-01-09 01:10:24.721979: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file /home/rich/src/concretely/concretely_nmt/nmt_model/vocab.to is already initialized.\n",
      "2019-01-09 01:10:24.721980: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file /home/rich/src/concretely/concretely_nmt/nmt_model/vocab.to is already initialized.\n",
      "2019-01-09 01:10:24.721980: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file /home/rich/src/concretely/concretely_nmt/nmt_model/vocab.from is already initialized.\n",
      "2019-01-09 01:10:24.825978: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file /home/rich/src/concretely/concretely_nmt/nmt_model/vocab.to is already initialized.\n",
      "2019-01-09 01:10:24.825978: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file /home/rich/src/concretely/concretely_nmt/nmt_model/vocab.to is already initialized.\n",
      "2019-01-09 01:10:24.825982: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file /home/rich/src/concretely/concretely_nmt/nmt_model/vocab.from is already initialized.\n",
      "2019-01-09 01:10:25.373578: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file /home/rich/src/concretely/concretely_nmt/nmt_model/vocab.to is already initialized.\n",
      "2019-01-09 01:10:25.373578: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file /home/rich/src/concretely/concretely_nmt/nmt_model/vocab.from is already initialized.\n",
      "2019-01-09 01:10:25.373578: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file /home/rich/src/concretely/concretely_nmt/nmt_model/vocab.to is already initialized.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    ". ./env.sh\n",
    "cd $NMT_DIR\n",
    "\n",
    "python -m nmt.nmt \\\n",
    "    --src=from --tgt=to \\\n",
    "    --vocab_prefix=$DATA_DIR/vocab  \\\n",
    "    --train_prefix=$DATA_DIR/train \\\n",
    "    --dev_prefix=$DATA_DIR/tst2012  \\\n",
    "    --test_prefix=$DATA_DIR/tst2013 \\\n",
    "    --out_dir=$MODEL_DIR \\\n",
    "    --num_train_steps=5 \\\n",
    "    --steps_per_stats=100 \\\n",
    "    --num_layers=2 \\\n",
    "    --num_units=128 \\\n",
    "    --dropout=0.2 \\\n",
    "    --metrics=bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    ". ./env.sh\n",
    "cd $HOME_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
